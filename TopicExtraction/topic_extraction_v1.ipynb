{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def preprossing_data(data):\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(data)):\n",
    "        data[idx] = data[idx].lower()  # Convert to lowercase.\n",
    "        data[idx] = tokenizer.tokenize(data[idx])  # Split into words.\n",
    "\n",
    "    # Remove pure numbers.\n",
    "    data = [[token for token in doc if not token.isnumeric()] for doc in data]\n",
    "    # Remove one character word.\n",
    "    data = [[token for token in doc if len(token) > 1] for doc in data]\n",
    "    # Remove stop words.\n",
    "    stop_words = stopwords.words('english')\n",
    "    data = [[word for word in doc if word not in stop_words] for doc in data]\n",
    "    # Remove the words that i think is meaningless\n",
    "    my_stop_words = ['rt','http']\n",
    "    data = [[word for word in doc if word not in my_stop_words] for doc in data]\n",
    "    # Lemmatize the documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data = [[lemmatizer.lemmatize(token) for token in doc] for doc in data]\n",
    "    \n",
    "    # Compute bigrams.\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "    bigram = Phrases(data, min_count=20)\n",
    "    for idx in range(len(data)):\n",
    "        for token in bigram[data[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                data[idx].append(token)\n",
    "                \n",
    "    return data\n",
    "\n",
    "\n",
    "def train_model(corpus, id2word, chunksize, iterations, num_topics, passes): \n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every= None\n",
    "    )\n",
    "    \n",
    "    # save model \n",
    "    # model.save('LDA_model_v1.model')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file parameters.\n",
    "file_path = \"tweets.csv\"\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 8000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "\n",
    "\n",
    "# read in file\n",
    "data = pd.read_csv(file_path)\n",
    "tweetslist = data['tweet '].values\n",
    "#print(len(tweetslist))\n",
    "tweetslist = preprossing_data(tweetslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(tweetslist)\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "\n",
    "# Save dict\n",
    "dictionary.save_as_text(\"my_dictionary.txt\")\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tweetslist]\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "\n",
    "# Get train model\n",
    "model = train_model(corpus, id2word, chunksize, iterations, num_topics, passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025*\"work\"\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "test = tweetslist[51]\n",
    "bow = dictionary.doc2bow(test)\n",
    "result = model.get_document_topics(bow)\n",
    "result = sorted(result, key=itemgetter(1), reverse=True)\n",
    "print(model.print_topic(result[0][0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mrdigitalafrica', 'really', 'need', 'design', 'thinking', 'mindset', 'entrepreneurial', 'journey', 'encourage', 'entrepreneur', 'take', 'part', 'design_thinking']\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
